# -*- coding: utf-8 -*-
"""explain vit_base_patch14_reg4_dinov2_activation_attention_maps.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/19NEPQYH-z5T_qp2QhqQTl72HBNPqGZxo

## Import Libraries
"""

import sys
import os
import requests
from PIL import Image
from torchvision import transforms
import numpy as np
import matplotlib.pyplot as plt
import torch
import torch.nn as nn
from torch.nn import functional as F
import timm
from timm.models.vision_transformer import VisionTransformer
from timm.data import resolve_data_config
from timm.data.transforms_factory import create_transform
import datasets
from collections import OrderedDict
import math
import inspect
import re
import cv2

from imagenet_labels import CLS2IDX

from chefer_explain.baselines.ViT.ViT_LRP import vit_base_patch16_224 as vit_LRP
from chefer_explain.baselines.ViT.ViT_LRP import vit_base_patch14_reg4_dinov2
from chefer_explain.baselines.ViT.ViT_LRP import deit3_base_patch16_224
from chefer_explain.baselines.ViT.ViT_LRP import deit3_small_patch16_224
# from baselines.ViT.ViT_LRP import deit3_medium_patch16_224
# from baselines.ViT.ViT_LRP import deit3_large_patch16_224
from chefer_explain.baselines.ViT.ViT_LRP import compute_rollout_attention as compute_rollout_attention_chefer

# Set seed for reproducibility
torch.manual_seed(0)
np.random.seed(0)


device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
# if torch.backends.mps.is_available():
#     device = torch.device("mps")

device = "cpu"

results_dir = "results"
# model_id = 


# model_id = "vit_large_patch14_224"
# model_id = "vit_large_patch16_rope_224"
# model_id = "vit_huge_patch16_224"


TARGET_CLASS = None  # if none use top-1 predicted class
# TARGET_CLASS = 282  

# model_id = "deit3_small_patch16_224.fb_in22k_ft_in1k"
# model_id = "deit3_base_patch16_224.fb_in22k_ft_in1k"
# model_id = "deit3_medium_patch16_224.fb_in22k_ft_in1k"

model_id_list = [
    "deit3_small_patch16_224.fb_in22k_ft_in1k", # done
    "deit3_base_patch16_224.fb_in22k_ft_in1k", # done
    "deit3_medium_patch16_224.fb_in22k_ft_in1k",
    "deit3_large_patch16_224.fb_in22k_ft_in1k",

    "deit3_large_patch16_384.fb_in22k_ft_in1k", # to compare image resolution
    "deit3_small_patch16_384.fb_in22k_ft_in1k",
    "deit3_base_patch16_384.fb_in22k_ft_in1k"

]

model_id_dict = {
    "vit_base_pattch16_224.augreg2_in21k_ft_in1k":{
        "extra_tokens": 1, # CLS token
    },
    "vit_base_patch14_reg4_dinov2.lvd142m":{
        "extra_tokens": 5, # 1 CLS + 4 registers + 1369 patches
    }
}

results_dir = f"{results_dir}/{model_id}"
os.makedirs(results_dir, exist_ok=True)

"""## Load All the images"""

os.listdir()

os.listdir("Sample Images")

lena_img_path = "Sample Images/Lena_image.png"
# cat_dog_img_path = "Sample Images/cat_dog_image.png"
cat_dog_png_hila_chefer_img_path = "Sample Images/catdog.png"
car_img_path = "Sample Images/car_image.jpg"

lena_img_np = np.array(Image.open(lena_img_path))
# cat_dog_img_np = np.array(Image.open(cat_dog_img_path))
cat_dog_img_np = np.array(Image.open(cat_dog_png_hila_chefer_img_path))
car_img_np = np.array(Image.open(car_img_path))

import matplotlib.pyplot as plt

fig, axes = plt.subplots(1, 3, figsize=(15, 5))

axes[0].imshow(lena_img_np)
axes[0].set_title('Lena Image')
axes[0].axis('on')

axes[1].imshow(cat_dog_img_np)
axes[1].set_title('Cat Dog Image')
axes[1].axis('on')

axes[2].imshow(car_img_np)
axes[2].set_title('Car Image')
axes[2].axis('on')

plt.tight_layout()
plt.savefig(f"{results_dir}/0_original_test_images.jpg")
plt.close(fig)  # Close the figure to avoid showing it

"""## Load the model"""


# model_id = "vit_base_patch14_dinov2.lvd142m"
model = timm.create_model(model_id, pretrained=True)
model.eval()

# Check for the parameter by name
# if hasattr(model, 'reg_token'):
#     print(f"Registers found: {model.reg_token.shape}")
# In timm, it is often named 'reg_token' or similar depending on the version

for name, param in model.named_parameters():
    if "cls" in name:
      print(name, param.shape)
    if "reg" in name:
      print(name, param.shape)

print("model architecture")
print(model)

# Write the modle architecture to a file
with open(f"{results_dir}/model_architecture.txt", "w") as f:
    f.write(str(model))

HAS_REGISTERS = False
NUMBER_OF_REGISTERS = 0
for name, param in model.named_parameters():
    if "reg" in name:
      print(name, param.shape)
      HAS_REGISTERS = True
      NUMBER_OF_REGISTERS = param.shape[1]

print(f"{HAS_REGISTERS=}\n{NUMBER_OF_REGISTERS=}")

"""## Preprocess images"""

# get model specific transforms (normalization, resize)
data_config = timm.data.resolve_model_data_config(model)
model_transforms = timm.data.create_transform(**data_config, is_training=False)

lena_img_tensor = model_transforms(Image.open(lena_img_path))
# cat_dog_img_tensor = model_transforms(Image.open(cat_dog_img_path))
cat_dog_img_tensor = model_transforms(Image.open(cat_dog_png_hila_chefer_img_path))
car_img_tensor = model_transforms(Image.open(car_img_path))



"""## Load the finetuned head

We are using DinoV2 backbone, and the classifier is randomly intiailized, we need to train this head.

If we are going to finetune the head, we need to do this before adding hooks, otherwise the training will take much longer because of the additional computations in hooks in each forward pass.

"""

NUM_CLASSES = len(CLS2IDX)  # 1000


if model_id == "vit_base_patch14_reg4_dinov2.lvd142m":
    # First, replace the head to match the checkpoint architecture
    model.head = nn.Linear(
        in_features=model.blocks[-1].mlp.fc2.out_features,  # 768
        out_features=NUM_CLASSES,
        bias=True
    )

    # Load the trained weights
    checkpoint_dir = f"checkpoints/{model_id}_finetuned_best.pth"
    model.load_state_dict(torch.load(checkpoint_dir, map_location='cpu'))

    print(f"Classification head for {model_id} loaded successfully")

# Move to device and set to eval mode

model.to(device)
model.eval()

model.model_id = model_id



"""## Hooks to capture activations"""

activations_od = OrderedDict()

len(model._modules['blocks'])

model._modules['blocks'][11]

"""Here I am trying to capture activation, for a forward hook there are three inputs, the module the hook is applied to, input tensor to that module, and the output of that module, since we are need activation we need the output tensor of each layer.

Before writing the hook, you need to examine the model architecture

I am using hook_handles to keep track of the hooks that I register. At the end of the entire pipeline, I can remove the hooks easily.

If I don't remove hooks, each time I run this below code cell, new hook will be registerd resulting in duplicate outputs.

Since each hook adds computation, depending on the processing you do inside the hook, hooks can slow down inference/training
"""

hook_handles = []

def get_activations(name:str):
  def activation_hook(module, input, output):
    activations_od[name] = output.detach()
  return activation_hook

# Add hooks to all the blocks to layerwise activations
for i, block in enumerate(model._modules['blocks']):
  h = block.register_forward_hook(get_activations(f"block_{i}"))
  hook_handles.append(h)

"""## Hooks to capture Attention Maps

Attention Map is the most important place when it comes to information flow. Because information mixing (information from one token to another token flows throught attention maps) happens.

\text{Softmax}(Q K^\top)
"""

model._modules['blocks'][11].attn

# print(inspect.getsource(model._modules['blocks'][11].attn.forward))

model._modules['blocks'][11].attn.fused_attn

model._modules['blocks'][11].attn.fused_attn = False

"""Since `self.fused_attn` is `True`, we can't capture attention map directly. (We can do it by manually setting for all the layer_idx, `model._modules['blocks'][layer_idx].attn.fused_attn = False` and then directly capture `attn`, but for this example let's consider the worst case, when you can't capture attention map directly.)

This `F.scaled_dot_product_attention` does not inherit from `nn.Module()`, therefore we cannot define hooks for this

This is the line responsible for calculating q, k embeddings before calculating attention.

```q, k = self.q_norm(q), self.k_norm(k)```
"""

model._modules['blocks'][11].attn.q_norm

model._modules['blocks'][11].attn.k_norm

"""If we register hooks for these layers, we can capture q, k embeddings separately. Then after some processing using

```python
q = q * self.scale
attn = q @ k.transpose(-2, -1)
```

We can get the attention map.
"""

attention_maps_od = OrderedDict()
q_embeddings_od = OrderedDict()
k_embeddings_od = OrderedDict()

type(model._modules['blocks'][11].attn.scale), model._modules['blocks'][11].attn.scale

def get_query_activations(name:str, q_scale:float):
  def query_activation_hook(module, input, output):
    q_embeddings_od[name] = output.detach() * q_scale # Here we need to pass q_scale as well, just like in the source code
  return query_activation_hook

def get_key_activations(name:str):
  def key_activation_hook(module, input, output):
    k_embeddings_od[name] = output.detach()
  return key_activation_hook

# Add hooks to all the blocks to layerwise activations
for i, block in enumerate(model._modules['blocks']):
  attn_block = block._modules['attn']
  # print(attn_block)
  # print(attn_block.scale)


  h1 = attn_block.q_norm.register_forward_hook(get_query_activations(f"block_{i}", attn_block.scale))
  h2 = attn_block.k_norm.register_forward_hook(get_key_activations(f"block_{i}"))
  hook_handles.append(h1)
  hook_handles.append(h2)

"""I am going to add another hooks to compute the attention map, right after the attention map. It is certain that the MLP layer is called right after the attention map. This might not be the best way to compute attn_map, But I need to compute along the forward pass

Note that I have ignored the attention mask, because in our case we are using an image, no need to mask tokens.
"""

def get_attention_maps(name:str):
  def compute_attn_map_hook(module, input, output):
    print(name)
    q = q_embeddings_od[name]
    k = k_embeddings_od[name]
    print("\tq shape", q.shape)
    print("\tk shape", k.shape)
    attn = q @ k.transpose(-2, -1)
    print("\tAttention Map shape before softmax", attn.shape)
    attention_maps_od[name] = attn.softmax(dim=-1)
    print("\tAttention Map shape after softmax", attention_maps_od[name].shape)


  return compute_attn_map_hook

# Add hooks to trigger computing the attenion maps
for i, block in enumerate(model._modules['blocks']):

  mlp_block = block._modules['mlp']
  # Forward function of 'MLP' block is guranteed to call after Attention block, There are some implementations
  # layer(x) = MLP(x) + Attn(x) but in our case layer(x) = MLP(Attn(x)) ( I found this implementation in Moondream VLM model,
  # so please inspect carefully before using this way)

  h = mlp_block.register_forward_hook(get_attention_maps(f"block_{i}"))
  hook_handles.append(h)

"""## Inference"""

x = cat_dog_img_tensor.unsqueeze(0).to(device)
with torch.inference_mode():
  output = model(x)



output.shape

predictions = output

def print_top_classes(predictions, **kwargs):
    # Print Top-5 predictions
    prob = torch.softmax(predictions, dim=1)
    class_indices = predictions.data.topk(5, dim=1)[1][0].tolist()
    max_str_len = 0
    class_names = []
    for cls_idx in class_indices:
        class_names.append(CLS2IDX[cls_idx])
        if len(CLS2IDX[cls_idx]) > max_str_len:
            max_str_len = len(CLS2IDX[cls_idx])

    print('Top 5 classes:')
    for cls_idx in class_indices:
        output_string = '\t{} : {}'.format(cls_idx, CLS2IDX[cls_idx])
        output_string += ' ' * (max_str_len - len(CLS2IDX[cls_idx])) + '\t\t'
        output_string += 'value = {:.3f}\t prob = {:.1f}%'.format(predictions[0, cls_idx], 100 * prob[0, cls_idx])
        print(output_string)

print_top_classes(output)

"""## Sanity Check"""

model_transforms_dict = timm.data.resolve_model_data_config(model)
model_transforms_dict

import torch
import matplotlib.pyplot as plt


def visualize_tensor(x, normalized=True):
    print("Input shape:", x.shape)

    # Remove batch dimension if present
    if x.dim() == 4 and x.shape[0] == 1:
        x = x[0]

    # Remove extra dim if shaped like (1, 3, 224, 224)
    while x.dim() > 3:
        x = x.squeeze(0)

    # Now x must be (3, H, W)
    if x.dim() != 3:
        raise ValueError(f"Expected 3D image tensor, got {x.dim()}D")

    # Denorm
    if normalized:
        mean = torch.tensor(model_transforms_dict['mean']).view(3,1,1).to(x.device)
        std  = torch.tensor(model_transforms_dict['std']).view(3,1,1).to(x.device)
        x = x * std + mean

    img = x.permute(1, 2, 0).clamp(0,1).cpu().numpy()

    plt.imshow(img)
    plt.axis("on")

    plt.savefig(f"{results_dir}/1_visualize_tensor.jpg")
    plt.close(fig)  # Close the figure to avoid showing it


def sanity_check(x, predictions, categories=None, normalized=True):
    print("=== INPUT CHECK ===")
    print("shape:", x.shape)                          # (1, 3, 224, 224)
    print("dtype:", x.dtype)
    print("min / max / mean:", x.min().item(), x.max().item(), x.mean().item())
    print("contains NaNs:", torch.isnan(x).any().item())

    print("\n=== MODEL PREDICTIONS CHECK ===")
    print("predictions shape:", predictions.shape)              # (1, 1000)
    print("predictions contains NaNs:", torch.isnan(predictions).any().item())

    probs = torch.softmax(predictions, dim=1)
    print("softmax sum:", probs.sum().item())         # should be ~1

    top_prob, top_idx = probs.topk(5)
    print("\n=== TOP-5 PREDICTIONS ===")
    class_indices = predictions.data.topk(5, dim=1)[1][0].tolist()
    max_str_len = 0
    class_names = []
    for cls_idx in class_indices:
        class_names.append(CLS2IDX[cls_idx])
        if len(CLS2IDX[cls_idx]) > max_str_len:
            max_str_len = len(CLS2IDX[cls_idx])

    print('Top 5 classes:')
    print('Top 5 classes:')
    output_string = ""
    for cls_idx in class_indices:
        line = '\t{} : {}'.format(cls_idx, CLS2IDX[cls_idx])
        line += ' ' * (max_str_len - len(CLS2IDX[cls_idx])) + '\t\t'
        line += 'value = {:.3f}\t prob = {:.1f}%\n'.format(predictions[0, cls_idx], 100 * probs[0, cls_idx])
        print(line.strip())
        output_string += line

    # Save the predictions
    with open(f"{results_dir}/00_predictions.txt", "w") as f:
        f.write(output_string)

    print("\n=== VISUALIZATION ===")
    visualize_tensor(x, normalized=normalized)

sanity_check(x, predictions=output, normalized=True)

"""## Delete Hooks"""

for h in hook_handles:
    h.remove()

"""## Process Hooks Outputs"""



"""## Visualize Activations

### Activation Visualization for one layer
"""

# Remove batch dimension & Removd [CLS] token & take mean so that I can get one 1 vector
activation_patches = activations_od['block_0'].squeeze(0)[1+NUMBER_OF_REGISTERS:,:].mean(dim=-1)
activation_patches.shape # [196]

NUM_PATCHES = int(math.sqrt(activation_patches.shape[0]))
NUM_PATCHES # 14

IMG_SIZE = lena_img_tensor.shape[-1] # 518

PATCH_SIZE = int(activation_patches.shape[0] / NUM_PATCHES) # 16

activation_patches.shape

NUM_PATCHES

# Reshape to 2D grid
activation_grid = activation_patches.view(NUM_PATCHES, NUM_PATCHES)  # [14, 14]

INTERPOLATION_METHOD = 'nearest'  # ['nearest' , 'bilinear' ]

activation_map = activation_grid.unsqueeze(0).unsqueeze(0)  # [1, 1, 14, 14]

if INTERPOLATION_METHOD == 'nearest':
  # Upsample to image size using nearest neighbor (each value becomes 16x16 block)
  activation_resized = F.interpolate(
      activation_map,
      size=(IMG_SIZE, IMG_SIZE),  # [224, 224]
      mode='nearest'
  ) # [1, 1, 224, 224]

elif INTERPOLATION_METHOD == 'bilinear':
  activation_resized = F.interpolate(
    activation_map,
    size=(IMG_SIZE, IMG_SIZE),  # [224, 224]
    mode='bilinear',
    align_corners=False
)

activation_resized = activation_resized.squeeze()  # [224, 224]

activation_resized.shape

def visualize_activation_overlay(x, activation_map, normalized=True, alpha=0.5, cmap='jet'):
    """
    Overlay activation heatmap on original image

    Args:
        x: Image tensor (1, 3, 224, 224) or (3, 224, 224)
        activation_map: Activation heatmap (224, 224)
        normalized: Whether x is ImageNet normalized
        alpha: Transparency of heatmap (0=invisible, 1=opaque)
        cmap: Colormap for heatmap ('jet', 'viridis', 'hot', etc.)
    """
    # Process image
    if x.dim() == 4 and x.shape[0] == 1:
        x = x[0]

    # Denormalize if needed
    if normalized:
        mean = torch.tensor(model_transforms_dict['mean']).view(3,1,1).to(x.device)
        std  = torch.tensor(model_transforms_dict['std']).view(3,1,1).to(x.device)
        x = x * std + mean

    # Convert to numpy (H, W, 3)
    img = x.permute(1, 2, 0).clamp(0, 1).cpu().numpy()

    # Normalize activation map to [0, 1]
    act = activation_map.cpu().numpy()
    act = (act - act.min()) / (act.max() - act.min() + 1e-8)

    # Create figure with subplots
    fig, axes = plt.subplots(1, 3, figsize=(15, 5))

    # Original image
    axes[0].imshow(img)
    axes[0].set_title('Original Image')
    axes[0].axis('off')

    # Heatmap only
    heatmap = axes[1].imshow(act, cmap=cmap)
    axes[1].set_title('Activation Heatmap')
    axes[1].axis('off')
    plt.colorbar(heatmap, ax=axes[1], fraction=0.046)

    # Overlay
    axes[2].imshow(img)
    axes[2].imshow(act, cmap=cmap, alpha=alpha)
    axes[2].set_title(f'Overlay (alpha={alpha})')
    axes[2].axis('off')

    plt.tight_layout()
    # plt.show()
    plt.savefig(f"{results_dir}/2_visualize_activation_overlay_one_layer.jpg")
    plt.close(fig)  # Close the figure to avoid showing it


visualize_activation_overlay(x, activation_resized, normalized=True, alpha=0.5)

"""### Activation Visualization over layers"""

import matplotlib.pyplot as plt
import torch
import torch.nn.functional as F
import math

def visualize_all_layers(x, activations_od, normalized=True, alpha=0.5, cmap='jet', interpolation='bilinear'):
    """
    Visualize activation heatmaps for all layers

    Args:
        x: Image tensor (1, 3, 224, 224)
        activations_od: OrderedDict with activations from all layers
        normalized: Whether x is ImageNet normalized
        alpha: Transparency of overlay
        cmap: Colormap for heatmap
        interpolation: 'nearest' or 'bilinear'
    """
    # Process original image once
    if x.dim() == 4 and x.shape[0] == 1:
        x_img = x[0]

    if normalized:
        x_img = model_transforms(x_img)

    img = x_img.permute(1, 2, 0).clamp(0, 1).cpu().numpy()

    # Get number of layers
    num_layers = len(activations_od)

    # Create figure: 2 rows (heatmap, overlay) x num_layers columns
    fig, axes = plt.subplots(2, num_layers, figsize=(4 * num_layers, 8))

    # Handle single layer case
    if num_layers == 1:
        axes = axes.reshape(2, 1)

    for idx, (layer_name, activation_tensor) in enumerate(activations_od.items()):
        # Process activation
        # Remove batch dimension & Remove [CLS] token & take mean
        activation_patches = activation_tensor.squeeze(0)[1+NUMBER_OF_REGISTERS:, :].mean(dim=-1)  # [196] or [N]


        NUM_PATCHES = int(math.sqrt(activation_patches.shape[0]))
        IMG_SIZE = x.shape[-1]  # 224

        # Reshape to 2D grid
        activation_grid = activation_patches.view(NUM_PATCHES, NUM_PATCHES)

        # Upsample to image size
        activation_map = activation_grid.unsqueeze(0).unsqueeze(0)

        if interpolation == 'nearest':
            activation_resized = F.interpolate(
                activation_map,
                size=(IMG_SIZE, IMG_SIZE),
                mode='nearest'
            )
        else:  # bilinear
            activation_resized = F.interpolate(
                activation_map,
                size=(IMG_SIZE, IMG_SIZE),
                mode='bilinear',
                align_corners=False
            )

        activation_resized = activation_resized.squeeze()  # [224, 224]

        # Normalize activation to [0, 1]
        act = activation_resized.cpu().numpy()
        act = (act - act.min()) / (act.max() - act.min() + 1e-8)

        # Row 0: Heatmap only
        im = axes[0, idx].imshow(act, cmap=cmap)
        axes[0, idx].set_title(f'{layer_name}\nHeatmap')
        axes[0, idx].axis('on')
        plt.colorbar(im, ax=axes[0, idx], fraction=0.046)

        # Row 1: Overlay
        axes[1, idx].imshow(img)
        axes[1, idx].imshow(act, cmap=cmap, alpha=alpha)
        axes[1, idx].set_title(f'{layer_name}\nOverlay')
        axes[1, idx].axis('on')

    plt.tight_layout()
    fig.suptitle(f"{model.model_id}; Activation Maps", fontsize=16)
    plt.tight_layout(rect=[0, 0, 1, 0.97])  # adjust so title doesn't overlap
    # plt.show()
    plt.savefig(f"{results_dir}/3_visualize_all_layers_activations.jpg")
    plt.close(fig)  # Close the figure to avoid showing it

visualize_all_layers(x, activations_od, normalized=True, alpha=0.6, interpolation='nearest') # Here there is a bug in mean error

# Check activation statistics for each layer
import matplotlib.pyplot as plt

# Store stats
layers = []
means = []
stds = []
mins = []
maxs = []
ranges = []

for layer_name, activation_tensor in activations_od.items():
    activation_patches = activation_tensor.squeeze(0)[1+NUMBER_OF_REGISTERS:, :].mean(dim=-1)

    m = activation_patches.mean().item()
    s = activation_patches.std().item()
    mn = activation_patches.min().item()
    mx = activation_patches.max().item()
    rg = mx - mn

    print(f"\n{layer_name}:")
    print(f"  Mean: {m:.4f}")
    print(f"  Std:  {s:.4f}")
    print(f"  Min:  {mn:.4f}")
    print(f"  Max:  {mx:.4f}")
    print(f"  Range: {rg:.4f}")

    layers.append(layer_name)
    means.append(m)
    stds.append(s)
    mins.append(mn)
    maxs.append(mx)
    ranges.append(rg)

# Plot
plt.figure(figsize=(12, 6))
plt.plot(layers, means, label="Mean")
plt.plot(layers, stds, label="Std")
plt.plot(layers, mins, label="Min")
plt.plot(layers, maxs, label="Max")
plt.plot(layers, ranges, label="Range")
plt.xticks(rotation=70)
plt.xlabel("Layer")
plt.ylabel("Value")
plt.title(f"{model.model_id}; Activation Statistics per Layer")
plt.legend()
plt.tight_layout()
# plt.show()
plt.savefig(f"{results_dir}/4_visualize_activation_statistics_per_layer.jpg")
plt.close(fig)  # Close the figure to avoid showing it

"""## Visualize Attention Maps

"""

len(attention_maps_od)

attention_maps_od['block_0'].shape

model.patch_embed

model.blocks[0].attn.num_heads

"""When image is processed in ViT-Tiny to get patch embeddings, `Conv2d` layer is used with 192 filters with 3 kernels per each filter (16x16) with stride 16.

So in the later layers, we have 192 features to process. In each layer, there are `3` heads. Therefore each head is responsible for processing 192/3 = 64 features. This results in one attention map per each attention head.

Notice in `attention_maps_od['block_0'].shape` = (1, 3, 197, 197) `3` is the number of attention maps (number of attention heads) per layer, and `197` refers to the number of tokens.

197 = [CLS] token + 196 image Patch tokens (In ViT)

If we considered DeiT model, there should be another token responsible for distillation.

### Visualize Attention Map of a Single Layer
"""

# Sanity check to see whether the sum is 1 along the row axis (Considering only one attention head)
attention_maps_od['block_0'].squeeze(0)[0, 0, :].sum()

# Get the mean about heads
attention_maps_od['block_0'].squeeze(0)[:, :, :].mean(dim=0).shape

# Only consider the first row because first row is "How [CLS] token attends to itself and other image patch tokens"
attention_maps_od['block_0'].squeeze(0)[:, : , :].mean(dim=0)[0,:].shape

# Now we can remove the first element of this row, that elements shows how [CLS] token attends to itself
attn_slice = attention_maps_od['block_0'].squeeze(0)[:, : , :].mean(dim=0)[0,1+NUMBER_OF_REGISTERS:]
attn_slice.shape

# now I am going to use softmax so that we will get a new probability distribtution after removing the first element
attn_slice = F.softmax(attn_slice, dim=0)
attn_slice.shape

NUM_PATCHES = int(math.sqrt(attention_maps_od['block_0'].squeeze(0)[:, : , :].mean(dim=0)[0,1:].shape[0]))
attn_slice = attn_slice.view(NUM_PATCHES, NUM_PATCHES)
attn_slice.shape

for k, v in attention_maps_od.items():
    print(k, v.shape)

def visualize_attention_overlay_simple(x, attn_slice, normalized=True, alpha=0.6, cmap='jet', interpolation='nearest', save_path=None, 
                                       model_id="", method_name="", cls_id=None, cls_name=""):
    """Simple single-plot overlay"""
    # Process image
    if x.dim() == 4 and x.shape[0] == 1:
        x_img = x[0]

    if normalized:
        mean = torch.tensor(model_transforms_dict['mean']).view(3, 1, 1).to(x_img.device)
        std = torch.tensor(model_transforms_dict['std']).view(3, 1, 1).to(x_img.device)
        x_img = x_img * std + mean

    img = x_img.permute(1, 2, 0).clamp(0, 1).cpu().numpy()

    # Upsample attention
    IMG_SIZE = x.shape[-1]
    attn_map = attn_slice.unsqueeze(0).unsqueeze(0)

    if interpolation == 'bilinear':
        attn_resized = F.interpolate(attn_map, size=(IMG_SIZE, IMG_SIZE),
                                     mode='bilinear', align_corners=False)
    elif interpolation == 'nearest':
        attn_resized = F.interpolate(attn_map, size=(IMG_SIZE, IMG_SIZE), mode='nearest')

    attn_resized = attn_resized.squeeze().cpu().numpy()

    # Plot
    fig = plt.figure(figsize=(8, 9)) # Increased height for title
    plt.imshow(img)
    plt.imshow(attn_resized, cmap=cmap, alpha=alpha)
    plt.axis('off')
    plt.colorbar(fraction=0.046)
    
    title = f"Model: {model_id}\nMethod: {method_name} ({interpolation})"
    if cls_id is not None:
        title += f"\nClass: {cls_id} ({cls_name})"
    
    plt.title(title, fontsize=10)
    # plt.show()
    
    if save_path:
        plt.savefig(save_path, bbox_inches='tight')
    else:
        plt.savefig(f"{results_dir}/5_visualize_attention_overlay_simple.jpg", bbox_inches='tight')
    
    plt.close(fig)  # Close the figure to avoid showing it

attn_slice.shape

# Usage
visualize_attention_overlay_simple(x, attn_slice, alpha=0.6)

attn_slice.shape

# Check if attention slice values are properly normalized
print(f"Attention shape: {attn_slice.shape}")
print(f"Min: {attn_slice.min():.4f}")
print(f"Max: {attn_slice.max():.4f}")
print(f"Mean: {attn_slice.mean():.4f}")
print(f"Sum: {attn_slice.sum():.4f}")  # Should be ~1.0 if normalized across patches

"""### Visualize Attention Maps over all the layers"""
import torch
import torch.nn.functional as F
import matplotlib.pyplot as plt

def visualize_attention_grid(x, attention_maps_od, normalized=True, alpha=0.6, cmap='jet', interpolation='nearest'):
    """
    Visualize attention maps across layers in a grid.
    - Horizontal axis: layers
    - Vertical axis: top row = attention heatmap, bottom row = overlay with original image
    """
    # Process image
    x_img = x[0]
    if normalized:
        mean = torch.tensor(model_transforms_dict['mean']).view(3, 1, 1).to(x_img.device)
        std = torch.tensor(model_transforms_dict['std']).view(3, 1, 1).to(x_img.device)
        x_img = x_img * std + mean
    img = x_img.permute(1, 2, 0).clamp(0, 1).cpu().numpy()

    num_layers = len(attention_maps_od)
    fig, axes = plt.subplots(3, num_layers, figsize=(4*num_layers, 12))

    if num_layers == 1:
        axes = axes[:, None]  # ensure axes is 2D



    for col, (layer_name, attn_tensor) in enumerate(attention_maps_od.items()):


        # Average over heads
        row_id = 0 # only choose the first row;because that is where CLS attendance happens
        
        # Full attention row for graph (CLS -> All)
        attn_row_full = attn_tensor.squeeze(0).mean(dim=0)[row_id, :].cpu().numpy()

        attn_slice = attn_tensor.squeeze(0).mean(dim=0)[row_id, 1+NUMBER_OF_REGISTERS:]  # remove CLS token + REGISTER_TOKENS

        # Upsample
        N = int((attn_slice.shape[0])**0.5)
        attn_map = attn_slice.view(1, 1, N, N)

        print(attn_map.shape)


        if interpolation == 'bilinear':
            attn_resized = F.interpolate(attn_map, size=(x.shape[-1], x.shape[-1]),
                                         mode='bilinear', align_corners=False)
        else:
            attn_resized = F.interpolate(attn_map, size=(x.shape[-1], x.shape[-1]), mode='nearest')
        attn_resized = attn_resized.squeeze().cpu().numpy()


        # Top row: attention heatmap
        im = axes[0, col].imshow(attn_resized, cmap=cmap)
        axes[0, col].axis('on')
        axes[0, col].set_title(f'{layer_name} Heatmap', fontsize=10)
        plt.colorbar(im, ax=axes[0, col], fraction=0.046)

        # Middle row: overlay
        axes[1, col].imshow(img)
        axes[1, col].imshow(attn_resized, cmap=cmap, alpha=alpha)
        axes[1, col].axis('on')
        axes[1, col].set_title(f'{layer_name} Overlay', fontsize=10)
        
        # Bottom row: Attention Graph
        ax_graph = axes[2, col]
        
        # Indices
        cls_idx = [0]
        reg_indices = list(range(1, 1 + NUMBER_OF_REGISTERS))
        patch_indices = list(range(1 + NUMBER_OF_REGISTERS, len(attn_row_full)))
        
        # Plot Patches
        ax_graph.plot(patch_indices, attn_row_full[patch_indices], label='Patches', color='blue', alpha=0.6, linewidth=0.5)
        
        # Plot Registers
        if reg_indices:
            ax_graph.scatter(reg_indices, attn_row_full[reg_indices], label='Registers', color='red', s=20, zorder=5)
            
        # Plot CLS
        ax_graph.scatter(cls_idx, attn_row_full[cls_idx], label='CLS', color='green', s=30, marker='*', zorder=10)
        
        # Separator lines
        if reg_indices:
            ax_graph.axvline(x=0.5, color='gray', linestyle='--', linewidth=0.5) # Sep CLS and Reg
            ax_graph.axvline(x=reg_indices[-1] + 0.5, color='gray', linestyle='--', linewidth=0.5) # Sep Reg and Patches
        else:
            ax_graph.axvline(x=0.5, color='gray', linestyle='--', linewidth=0.5) # Sep CLS and Patches
            
        ax_graph.set_title(f'{layer_name} Raw Attn', fontsize=10)
        if col == 0:
            ax_graph.set_ylabel('Attention Weight')
            ax_graph.legend(fontsize=8)
        ax_graph.set_xlabel('Token Index')
        ax_graph.grid(True, alpha=0.3)

    fig.suptitle(f"{model.model_id}; [CLS] Token Attendance", fontsize=16)
    plt.tight_layout(rect=[0, 0, 1, 0.97])  # adjust so title doesn't overlap
    # plt.show()
    plt.savefig(f"{results_dir}/6_visualize_attention_grid.jpg")
    plt.close(fig)  # Close the figure to avoid showing it

l = visualize_attention_grid(x, attention_maps_od)


def compute_rollout_attention(all_layer_matrices, start_layer=0):
    # adding residual consideration
    num_tokens = all_layer_matrices[0].shape[1]
    batch_size = all_layer_matrices[0].shape[0]
    eye = torch.eye(num_tokens).expand(batch_size, num_tokens, num_tokens).to(all_layer_matrices[0].device)
    all_layer_matrices = [all_layer_matrices[i] + eye for i in range(len(all_layer_matrices))]
    # all_layer_matrices = [all_layer_matrices[i] / all_layer_matrices[i].sum(dim=-1, keepdim=True)
    #                       for i in range(len(all_layer_matrices))]
    joint_attention = all_layer_matrices[start_layer]
    for i in range(start_layer+1, len(all_layer_matrices)):
        joint_attention = all_layer_matrices[i].bmm(joint_attention)
    return joint_attention

# TODO: Visualize Attention Rollout Explainability Map

attn_cams = []
for layer_name, attn_tensor in attention_maps_od.items():
    # attn_tensor shape: (1, num_heads, num_tokens, num_tokens)
    # Average over heads
    avg_heads = attn_tensor.mean(dim=1) # (1, num_tokens, num_tokens)
    attn_cams.append(avg_heads)

# Compute rollout
rollout = compute_rollout_attention(attn_cams, start_layer=0)

# Extract [CLS] token attention to image patches
# rollout shape: (1, num_tokens, num_tokens)
# We want the first row (CLS attending to others), excluding the first column (CLS attending to itself) and register tokens
rollout_attn = rollout[:, 0, 1+NUMBER_OF_REGISTERS:] # (1, num_patches)

# Reshape to grid
NUM_PATCHES = int(math.sqrt(rollout_attn.shape[1]))
rollout_grid = rollout_attn.view(NUM_PATCHES, NUM_PATCHES)

# Visualize
# Visualize
# Attention Rollout is class-agnostic, so we just add model_id
visualize_attention_overlay_simple(x, rollout_grid, alpha=0.6, interpolation='nearest', 
                                   save_path=f"{results_dir}/7_visualize_attention_rollout_{model.model_id}_nearest.jpg",
                                   model_id=model.model_id, method_name="Attention Rollout")
visualize_attention_overlay_simple(x, rollout_grid, alpha=0.6, interpolation='bilinear', 
                                   save_path=f"{results_dir}/7_visualize_attention_rollout_{model.model_id}_bilinear.jpg",
                                   model_id=model.model_id, method_name="Attention Rollout")


## TODO: CheferCAM (Transformer Atribution/Grad) Explainability Map (I want to show this one)


print("Initializing LRP model...")
if model_id == "vit_base_patch14_reg4_dinov2.lvd142m":
    model_lrp = vit_base_patch14_reg4_dinov2(pretrained=False, img_size=518).to(device)
elif model_id == "deit3_base_patch16_224.fb_in22k_ft_in1k":
    model_lrp = deit3_base_patch16_224(pretrained=False).to(device)
elif model_id == "deit3_small_patch16_224.fb_in22k_ft_in1k":
    model_lrp = deit3_small_patch16_224(pretrained=False).to(device)
else:
    print(f"[DEBUG]Initializing LRP model for {model_id}")
    model_lrp = vit_LRP(pretrained=False).to(device)
model_lrp.eval()

# Transfer weights from timm model to LRP model
print("Transferring weights...")
timm_state_dict = model.state_dict()
    
lrp_state_dict = model_lrp.state_dict()
if 'pos_embed' in lrp_state_dict:
    print(f"lrp pos_embed shape: {lrp_state_dict['pos_embed'].shape}")



new_state_dict = {}
for key in lrp_state_dict.keys():
    if key in timm_state_dict:
        if key == 'pos_embed' and lrp_state_dict[key].shape != timm_state_dict[key].shape:
            print(f"Handling pos_embed mismatch: {lrp_state_dict[key].shape} vs {timm_state_dict[key].shape}")
            # Assuming timm pos_embed is just patches (1369) and lrp is CLS+REG+Patches (1374)
            # We copy timm patches to lrp patches
            # lrp pos_embed: [1, 1+num_reg+num_patches, 768]
            # timm pos_embed: [1, num_patches, 768]
            
            # Initialize with zeros for CLS and REG tokens (since timm doesn't seem to use pos_embed for them)
            # and copy patches from timm
            new_pos_embed = torch.zeros_like(lrp_state_dict[key])
            
            # Copy patches (last 1369 elements)
            # indices: 0=CLS, 1..4=REG, 5..=PATCHES

            
            num_extra_tokens = 1 + (4 if model_id == "vit_base_patch14_reg4_dinov2.lvd142m" else 0)
            new_pos_embed[:, num_extra_tokens:, :] = timm_state_dict[key]
            new_state_dict[key] = new_pos_embed
        else:
            new_state_dict[key] = timm_state_dict[key]
    else:
        # Handle potential naming mismatches if any
        # For example, timm might use 'head.weight' and LRP might use 'head.weight' (same)
        # But let's check for 'patch_embed.proj.weight' vs 'patch_embed.proj.weight'
        print(f"Warning: Key {key} not found in timm model")

# Load the transferred weightsx``
msg = model_lrp.load_state_dict(new_state_dict, strict=True)
print("Weight transfer result:", msg)

# Forward pass with LRP model
print("Running CheferCAM...")
output_lrp = model_lrp(x)
prediction = output_lrp.max(1)[1].item()
print(f"Prediction: {CLS2IDX[prediction]}")

# Backward pass to get gradients
model_lrp.zero_grad()

# You can change the target class here. 
# By default, we use the predicted class, but you can set it to any class index (0-999).
if TARGET_CLASS == None:
    TARGET_CLASS = prediction # default


# TARGET_CLASS = 285 # Example: Force explanation for 'Egyptian cat'

# We need to backpropagate the score of the target class
score = output_lrp[0, TARGET_CLASS]
score.backward()

# Relprop to get the CAM
# method="transformer_attribution" corresponds to the logic the user asked for
one_hot = torch.zeros((1, output_lrp.shape[-1]), device=device)
one_hot[0, TARGET_CLASS] = score
cam_lrp = model_lrp.relprop(one_hot, method="transformer_attribution", alpha=1)
cam_lrp = cam_lrp.detach().reshape(NUM_PATCHES, NUM_PATCHES) # Reshape to (14, 14) for visualization

# Visualize
# CheferCAM is class-specific, so we add model_id and cls_id
visualize_attention_overlay_simple(x, cam_lrp, alpha=0.6, interpolation='nearest', 
                                   save_path=f"{results_dir}/8_visualize_chefer_cam_{model.model_id}_{TARGET_CLASS}_nearest.jpg",
                                   model_id=model.model_id, method_name="CheferCAM (Transformer Attribution)", cls_id=TARGET_CLASS, cls_name=CLS2IDX[TARGET_CLASS])
visualize_attention_overlay_simple(x, cam_lrp, alpha=0.6, interpolation='bilinear', 
                                   save_path=f"{results_dir}/8_visualize_chefer_cam_{model.model_id}_{TARGET_CLASS}_bilinear.jpg",
                                   model_id=model.model_id, method_name="CheferCAM (Transformer Attribution)", cls_id=TARGET_CLASS, cls_name=CLS2IDX[TARGET_CLASS])




print("CheferCAM visualization saved.")


